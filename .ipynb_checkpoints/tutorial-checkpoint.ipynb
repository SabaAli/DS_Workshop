{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCFemTech Tour de Code: Data Science & Machine Learning Tutorial\n",
    "\n",
    "### Tutorial Overview\n",
    "This tutorial is divided into 2 parts. During the first half of the tutorial, we introduce Pandas and exploratory data analysis. The second half will cover more advanced topics such as Machine Learning.\n",
    "\n",
    "At any point during the tutorial if you get stuck or need further clarification, please do not hesitate to raise your hand so that a facilitator can help you.\n",
    "\n",
    "### Dataset\n",
    "The dataset `census.csv`  was derived from the Census Bureau Database and is a comprehensive record of over 48,000 individuals and their socio-economic information.\n",
    "\n",
    "### The Challenge\n",
    "Determine a person's income level based on the socio-economic measures given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning & Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Import the packages we'll be using during the exercise. This has already been done for you. Just select the cell and click \"Run\" (or Shift+Return on Macbooks) to execute the package installation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the packages you need. Simply run this cell.\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('max_rows', 999) # default is 60 rows\n",
    "pd.set_option('max_columns', 999) #default is 20 columns\n",
    "\n",
    "# Always set a random seed to replicate your results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your packages installed, we will load our census data from our csv (comma-separated value) file into a **Pandas DataFrame**. A dataframe is which is a 2-dimensional labeled data structure with columns of potentially different types. Each row has a unique label (the row index), and each column has a unique label (the column index). Simply stated, a dataframe is a table of data, similar to a spreadsheet or SQL table.\n",
    "\n",
    "A csv file is a text file containing data in table form, where columns are separated using the ‘,’ comma character, and rows are on separate lines. Loading a csv file is made extremely simple with the `.read_csv()` function in Pandas, once you know the path to your file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line to \"read-in\" your dataset\n",
    "census = pd.read_csv(\"datasets/census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've read in the data, use the `.head()` method, which prints the first 5 rows (by default). You can use this method to inspect just the beginning of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the first 5 rows of your dataset\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.head(20) # Prints the first 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see the last rows of your dataset? Using the `.tail()` method, inspect the end of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .tail() method to see the bottowm rows\n",
    "census.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.columns` to get a list of all the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.columns\n",
    "# where did the () go? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some colums have extra white space at the beginning or the end (race, income). Let's quickly remove these leading and trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our column names won't work. We strongly recommend removing dashes and only using underscores\n",
    "census.columns = census.columns.str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading or trailing whitespace from columns:\n",
    "census.columns = census.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.shape` command gives information on the size of the dataset. The first number is the number of rows and the second is the number of columns.\n",
    "\n",
    "#### Q. How many rows and columns are in the data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function here\n",
    "# Rows, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many dataframes have mixed data types, that is, some columns are integers, some are strings, and some are dates etc. Internally, csv files do not contain information on what data types are contained in each column. Pandas infers the data types when loading the data.\n",
    "\n",
    "Use the `.dtypes` method to check the types of each column.\n",
    "\n",
    "*Note: strings are loaded as ‘object’ datatypes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .dtypes method here\n",
    "census.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.describe()`, `.unique()`, and `.info()` to do more basic exploration before we begin cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter .describe(), .unique() and .info() here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a column of data using the name of the column as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to choose a column/series:\n",
    "census['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pandas, you can also use a . to select a column/series\n",
    "census.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[['age']] # Selects index and column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[['age','sex', 'education']] # Selects multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's practice with some exercises!\n",
    "#### Q. Print the occupation column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print occupation column:\n",
    "census[[\"occupation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Print the marital status, relationship, and age columns below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital status column/series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship column/series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age column/series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "It's exceedingly rare that a clean dataset will be available. Missing values in a dataset is always a problem in real life scenarios. Areas like machine learning and data mining face severe issues in the accuracy of their model predictions because of poor quality of data caused by missing values. In these areas, missing value treatment is a major point of focus to make their models more accurate and valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of nulls per column you can use the `.isnull()` and `.sum()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.isnull().sum() #sum of all the nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: You can also visualize this by plotting the number of nulls per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make a bar graph. And how to plot null values\n",
    "\n",
    "null_sum = census.isnull().sum()\n",
    "\n",
    "null_sum.plot(kind='bar') # Specifies a bar graph\n",
    "plt.title('Number of null values per column') # Creates a title for the graph\n",
    "plt.show() # Displays the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever need to remove a row or column to clean our data, we can use the `.drop()` function.\n",
    "\n",
    "Let's first try removing a column. To delete a column, use the name of the column, and specify the “axis” as 1 (rows are `axis=0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column titled \"native_country\" and preview the first 5 row.\n",
    "census.drop(\"native_country\", axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try removing a row. To delete a row, use the index labels, and specify the \"axis\" as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the rows with label 2 and preview the first 5 rows.\n",
    "census.drop([2], axis=0).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to remove rows which contain missing values, we can use the `.dropna()` function. We can also specify whether we want to drop a row if it contains all NA or at least one NA.\n",
    "\n",
    "* ‘any’ : If any NA values are present, drop that row or column.\n",
    "* ‘all’ : If all values are NA, drop that row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values. Then look at the shape to see what was dropped\n",
    "census = census.dropna(axis=0, how=\"any\")\n",
    "census.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that we undertand how to clean our data, let's dive deeper into the data to see what we can find. \n",
    "\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use built-in functions to find various types of summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.min() # Find the minimum age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.max() # Find the maximum age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.median() # Find the median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.mean() # Find the average age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Find the minimum, maximum, median, and mean hours per week.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum hours per week here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum hours per week here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median hours per week here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean hours per week here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "We can use simple operator comparisons on columns to extract relevant or drop irrelevant information. If we want to filter the data to see only the individuals who are less than 20 years of age we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find people lesss than 20 years old \n",
    "age_filter = census.age < 20 \n",
    "census[age_filter] # Will filter our dataframe by the condition we specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataframe to see only the individuals who work more than 40 hours per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_worked_filter = census[\"hours_per_week\"] > 40 # folks who work more than 40 hrs per week\n",
    "census[hours_worked_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try filtering for only females:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sex_filter = census.sex.str.strip() == 'Female' # in Python == means equal. Know the diference between = and ==\n",
    "census[sex_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[census.sex.str.strip() == 'Female'] # Same as above cell, condensed onto one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine multiple conditions as well by using the **&** operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[(census.age < 20) & (census.sex.str.strip() =='Female')] #folks who are under age 20 and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself. Work with the person next to you to create your own filter. Be prepared to share your filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own filter here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: More Exploratory Data Analysis (EDA)\n",
    "\n",
    "Often while working with Pandas dataframe you might have a column with categorical variables, string/characters, and you want to find the frequency counts of each unique elements present in the column. Pandas’ `.value_counts()` easily let you get the frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find value counts here\n",
    "census['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. How many folks have a 9th grade education?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try finding the number of individuals by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value counts of the native country.\n",
    "census['native_country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.groupby()` method is a very powerful Pandas method. \n",
    "You can group by one column and count the values of another column per this column value using the `.value_counts()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides a value count for marital_status by country\n",
    "census.groupby('native_country')['marital_status'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the groupby function in combination with statistical functions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gender, calculate the mean of the numeric columns in the dataframe.\n",
    "census.groupby('sex').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try grouping by age and find the mean of numeric columns in the dataframe. Do you notice any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby age and find mean of numeric columns in dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Basic Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now take a look at the Matplotlib package for visualization in Python. Matplotlib is a multi-platform data visualization library built on NumPy arrays. It provides both a very quick way to visualize data from Python and publication-quality figures in many formats. \n",
    "\n",
    "We can use the `.plot()` function to specificy the type of chart we want.\n",
    "\n",
    "The first type of visualization we can do is a bar chart/plot, which are best for showing numerical comparison across different categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of individuals at each education level\n",
    "census['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with bar plot\n",
    "census['education'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots can be used to show the relationship between two numerical variables. We can compare age and hours per week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.plot(kind='scatter', x='age', y='hours_per_week', alpha=0.1); # alpha variable controls transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram plot is generally used to summarize the distribution of a data sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = census.age.plot(kind='hist', bins=20, title='Histogram of Age'); # Creates the histogram\n",
    "ax.set_xlabel('Age'); # Sets the label for the x axis\n",
    "ax.set_ylabel('Frequency'); # Sets the label for the y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW IT'S YOUR TURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've walked through some of getting started with exploratory data analysis, spend time exploring the data and doing your own analysis.\n",
    "\n",
    "Use the cheat sheets and your facilitators, as well as help from the other participants, try to answer some of the following questions or create your own questions:\n",
    "\n",
    "* Find the average capital_gain and capital_loss.\n",
    "* Find only those individuals who have a Bachelors and are never married. (*Hint*: use filtering methods)\n",
    "* Optional: Visualize the number of individuals by relationship. (*Hint*: use a bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Machine Learning and Predictive Analysis\n",
    "\n",
    "What kind of predictive analysis can we do on this data? Which countries have incomes below or above $50k? \n",
    "\n",
    "Machine learning is a technique that uses the past to guess at what the future will hold. We can apply it here in a variety of ways. There are two main types of machine learning: supervised (where the target is labeled) and unsupervised (where the target is not labeled). We'll work primarily with supervised machine learning for this data set. Beyond that, there are several main tasks for the machines: categorizing data based on it's features or predicting out the future.\n",
    "\n",
    "![Python's ML Package, Scikit-learn has it's own cheat sheet](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "http://scikit-learn.org/stable/_static/ml_map.png\n",
    "\n",
    "You may have seen while doing your exploratory analysis that some trends are clearer than others. There's always a fuzzy area and machine learnining works to handle that ambiguity in ways that humans are not so great at. Of course, there's plenty of legal and ethical questions surrounding the use of machine learning, as input biases lead to output biases and creating fairness in artificial intelligence is a hot topic, but for now we'll use machines to see if we can draw any new insights from out data.\n",
    "\n",
    "We will be using **Scikit-learn**, a Python library for building machine learning models. Scikit-learn provides a range of supervised and unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the splitter and the Machine Learning (ML) algorithms we will use.\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import tree\n",
    "#from sklearn.cross_validation import KFold, cross_val_score\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Viewing the Data\n",
    "\n",
    "Before going into machine learning, we have to do some more exploratory data analysis to see what features in the machine make a difference to the outcome. Where there are poor correlations, we're likely to see poor performance, where as stronger correlations mean better performance. Below, we'll view the data two different ways -- using a pair plot and a correlation plot heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the relationship of our target variable (income) to each other variable. We are using the Seaborn library to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship of our target variable to each other variable:\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(census, hue=\"income\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a correlation matrix (.corr() method). \n",
    "# Need to know which features are/aren't correlated with each other. \n",
    "# Goal is to feed uncorrelated features into the model.\n",
    "\n",
    "corr = census.corr()\n",
    "sns.heatmap(corr, annot = True); #in seaborn, put a ; at the end to remove that funny line\n",
    "#annot = True means to place the values in each square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Some models only work with numeric data. That means we must convert all the words (categorical data) to numbers in order to feed it into our models. \n",
    "\n",
    "Let's get to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = pd.read_csv(\"datasets/census.csv\")\n",
    "census.columns = census.columns.str.strip()\n",
    "census.columns = census.columns.str.replace('-','_')\n",
    "census.dtypes\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas's `.get_dummies()` to convert categorical variable into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the categorical (non-numerical) columns into a new dataframe called \"dummy_columns\"\n",
    "dummy_columns = census[['work_class', 'income', 'marital_status', 'race', 'sex', 'relationship', 'native_country', 'education', 'occupation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummy_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a dataframe of our **numeric** columns. We'll merge this onto the dummy'd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_columns = census[['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we concatenate the numeric columns to the dummied column, and drop the first one.\n",
    "# This new dataframe is called \"df\"\n",
    "df = pd.concat([numeric_columns, pd.get_dummies(dummy_columns, dummy_na=True, drop_first=True)], axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's define a few key terms:\n",
    "\n",
    "**Features** are individual independent variables that act as the input in our model. The number of features are called dimensions. \n",
    "* We will define this as **X**.\n",
    "\n",
    "**Target** is whatever the output is of the input variables. It could be the individual classes that the input variables maybe mapped to in case of a classification problem or the output value range in a regression problem. If the training set is considered then the target is the training output values that will be considered.\n",
    "Simply put, it is the thing we are trying to predict. \n",
    "* We will define this as **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target variable. First.\n",
    "y = df['income_ >50K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, define all of the features you'll use in your model. Always drop the y variable.\n",
    "X = df.drop(['income_ >50K', 'income_nan'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This is one of the most commonly used models. Logistic regresssion allows us **to predict classes**, thus it is a type of *classifier*. In this case, we want to predict whether or not an individual's income is greater than 50k.\n",
    "\n",
    "First, we must split our data into a **training set** and **testing set**.\n",
    "* **training set**: the data that the algorithm will \"learn\" from. Includes the outcome (y variable).\n",
    "* **testing set**: also called the *validation set*, this is the data used to measure *how well* the model performs at making predictions on that test set. Does **not** include the outcome.\n",
    "\n",
    "\n",
    "Figuring out how much of your data should be split into your test set is a tricky question. If your training set is too small, then your algorithm might not have enough data to effectively learn. On the other hand, if your test set is too small, then your accuracy and precision could have a large variance. You might happen to get a really lucky or a really unlucky split! In general, putting 80% of your data in the training set, and 20% of your data in the test set is a good place to start.\n",
    "\n",
    "Using the `train_test_split` method, we generate 4 dataframes:\n",
    "* `X_train` is the training data set\n",
    "* `y_train` is the set of labels to all the data in `X_train`\n",
    "* `X_test` is the test data set\n",
    "* `y_test` is the set of labels to all the data in `X_test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit!\n",
    "Now it's time to fit our model on the **training data** using the `fit(X, y)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a score! \n",
    "\n",
    "Now let's score our model to see how well it performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training set accuracy: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've built a model. This here is machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make predictions!\n",
    "\n",
    "The `predict(X)` method, given unlabeled observations X (test data set) returns the predicted labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outcomes\n",
    "y_pred_class = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data is an array. We can convert this into a dataframe using the `pd.DataFrame()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.DataFrame(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a comparison of our predicted outcomes to the actual values in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.concat([outcomes, df])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Which features are more the most important in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Logistic regression coefficients\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(lr.coef_.T, 'o', label=\"C=1\")\n",
    "#plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "#plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(X.shape[1]), X, rotation=90)\n",
    "plt.hlines(0, 0, X.shape[1])\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.savefig('log_coef') # Creates a plot and saves the figure in your working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Random Forests\n",
    "\n",
    "Decision trees are a type of model used for both classification and regression. Trees answer sequential questions which send us down a certain route of the tree given the answer. The model behaves with “if this than that” conditions ultimately yielding a specific result.\n",
    "\n",
    "A Random Forest, a classifier, is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Now let's fit our model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's score our random forest model. Looks like this model is ... overfit (dun dun dun)\n",
    "print(\"Training set accuracy: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What features had the most difference in the training set?\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Your Turn:\n",
    "\n",
    "Look into various implementations of tree models and give them a try. Do any improve the quality of the data? Also consider removing features or using different feature combinations to optimize your results. Can you get above an 80% accuracy? What happens when you change the test/train split? There are many parameters that can be changed when performing machine learning. Which ones give you the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
